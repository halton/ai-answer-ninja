apiVersion: v1
kind: ConfigMap
metadata:
  name: conversation-analyzer-config
  namespace: ai-ninja
data:
  SERVICE_NAME: "conversation-analyzer"
  SERVICE_PORT: "3010"
  LOG_LEVEL: "INFO"
  DATABASE_URL: "postgresql://ai_ninja:ai_ninja_password@postgres:5432/ai_ninja"
  REDIS_URL: "redis://redis:6379"
  HUGGINGFACE_CACHE_DIR: "/app/cache/huggingface"
  SPACY_MODEL: "zh_core_web_sm"
  TORCH_DEVICE: "cpu"
  MAX_CONCURRENT_ANALYSES: "5"
  CACHE_TTL: "3600"
  BATCH_SIZE: "16"
  PROFILE_ANALYTICS_URL: "http://profile-analytics:3004"
  REALTIME_PROCESSOR_URL: "http://realtime-processor:3002"
  CONVERSATION_ENGINE_URL: "http://conversation-engine:3003"
  USER_MANAGEMENT_URL: "http://user-management:3005"
  ALLOWED_ORIGINS: "*"
  ENABLE_METRICS: "true"

---
apiVersion: v1
kind: Secret
metadata:
  name: conversation-analyzer-secrets
  namespace: ai-ninja
type: Opaque
stringData:
  AZURE_SPEECH_KEY: "your-azure-speech-key"
  AZURE_SPEECH_REGION: "your-azure-region"
  AZURE_OPENAI_ENDPOINT: "https://your-openai.openai.azure.com/"
  AZURE_OPENAI_API_KEY: "your-azure-openai-key"
  AZURE_OPENAI_DEPLOYMENT_NAME: "gpt-4"
  AZURE_STORAGE_ACCOUNT: "your-storage-account"
  AZURE_STORAGE_KEY: "your-storage-key"
  SECRET_KEY: "your-secret-key"
  SENTRY_DSN: "your-sentry-dsn"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: conversation-analyzer
  namespace: ai-ninja
  labels:
    app: conversation-analyzer
    version: v1
    tier: analysis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: conversation-analyzer
  template:
    metadata:
      labels:
        app: conversation-analyzer
        version: v1
        tier: analysis
    spec:
      containers:
      - name: conversation-analyzer
        image: conversation-analyzer:1.0.0
        ports:
        - containerPort: 3010
          name: http
          protocol: TCP
        envFrom:
        - configMapRef:
            name: conversation-analyzer-config
        - secretRef:
            name: conversation-analyzer-secrets
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          httpGet:
            path: /health
            port: 3010
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 3010
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - name: cache-volume
          mountPath: /app/cache
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: cache-volume
        emptyDir:
          sizeLimit: 10Gi
      - name: logs-volume
        emptyDir:
          sizeLimit: 5Gi
      restartPolicy: Always
      terminationGracePeriodSeconds: 30

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: conversation-analyzer-worker
  namespace: ai-ninja
  labels:
    app: conversation-analyzer-worker
    version: v1
    tier: analysis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: conversation-analyzer-worker
  template:
    metadata:
      labels:
        app: conversation-analyzer-worker
        version: v1
        tier: analysis
    spec:
      containers:
      - name: conversation-analyzer-worker
        image: conversation-analyzer:1.0.0
        command: ["python", "-c"]
        args:
        - |
          import asyncio
          from app.pipelines.realtime_processor import realtime_pipeline
          from app.core.database import db_manager
          from app.core.cache import cache_manager
          from app.core.logging import setup_logging
          
          async def main():
              setup_logging()
              await db_manager.initialize()
              await cache_manager.initialize()
              await realtime_pipeline.initialize()
              await realtime_pipeline.process_queued_tasks()
          
          asyncio.run(main())
        envFrom:
        - configMapRef:
            name: conversation-analyzer-config
        - secretRef:
            name: conversation-analyzer-secrets
        env:
        - name: SERVICE_NAME
          value: "conversation-analyzer-worker"
        - name: MAX_CONCURRENT_ANALYSES
          value: "3"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "6Gi"
            cpu: "3"
        volumeMounts:
        - name: cache-volume
          mountPath: /app/cache
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: cache-volume
        emptyDir:
          sizeLimit: 10Gi
      - name: logs-volume
        emptyDir:
          sizeLimit: 5Gi
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: conversation-analyzer
  namespace: ai-ninja
  labels:
    app: conversation-analyzer
    tier: analysis
spec:
  type: ClusterIP
  ports:
  - port: 3010
    targetPort: 3010
    protocol: TCP
    name: http
  selector:
    app: conversation-analyzer

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: conversation-analyzer-ingress
  namespace: ai-ninja
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  tls:
  - hosts:
    - conversation-analyzer.ai-ninja.com
    secretName: ai-ninja-tls
  rules:
  - host: conversation-analyzer.ai-ninja.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: conversation-analyzer
            port:
              number: 3010

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: conversation-analyzer-hpa
  namespace: ai-ninja
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: conversation-analyzer
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: conversation-analyzer-metrics
  namespace: ai-ninja
  labels:
    app: conversation-analyzer
    tier: analysis
spec:
  selector:
    matchLabels:
      app: conversation-analyzer
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s